{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio2Midi Transcription for Piano\n",
    "***\n",
    "\n",
    "ROLI has been developing a modular digital keyboard with RGB luminations to guide the users to learn and play with their favorite music. The interaction is designed so that the LUMI APP provides extensive tutorial vidoes and more generally song contents with meta musical information that could be displayed in real-time on the light up keyboards, which provides visual guidance and gamification elements to engage with learners at various experience level, as there's always more fun to learn with their favorite song track.\n",
    "\n",
    "<img src = \"./imgs/lumi.jpg\" style = \"width: 800px\"/>\n",
    "\n",
    "As an important element in realizing the desired user experience through LUMI and LUMI APP, machine-learning assisted music information retrieval (ML-MIR) is an active research area for me and my team at ROLI. The notebooks showcases some of basic work related to piano audio transcription. Based on some of the state of art work ([Reference 1 - An End-to-End Neural Network for Polyphonic Piano Music Transcirption](https://arxiv.org/abs/1508.01774) , [Reference 2 - Onsets and Frames: Dual-Objective Piano Transcription](https://arxiv.org/abs/1710.11153)) We've explored ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from magenta.common import tf_utils\n",
    "from magenta.music import audio_io\n",
    "import magenta.music as mm\n",
    "from magenta.models.onsets_frames_transcription import audio_label_data_utils\n",
    "from magenta.models.onsets_frames_transcription import configs\n",
    "from magenta.models.onsets_frames_transcription import constants\n",
    "from magenta.models.onsets_frames_transcription import data\n",
    "from magenta.models.onsets_frames_transcription import infer_util\n",
    "from magenta.models.onsets_frames_transcription import train_util\n",
    "from magenta.music import midi_io\n",
    "from magenta.protobuf import music_pb2\n",
    "from magenta.music import sequences_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The piano onset detector has been trained on MAPS (Disklavier) dataset, which consists of 17.9 hrs piano playing, which has been used as the test set while the training set is artificially generated from ROLI's equator synthesizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "<img src='./imgs/CRNN-Onset Stack.jpg' style = 'width:800px' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "+ Adam Optimizer for 20,000 epochs\n",
    "+ Learning rate warm up for 500 epochs\n",
    "+ 32 hrs on a single local GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ning/anaconda3/envs/audio2midi/lib/python3.7/site-packages/magenta/models/onsets_frames_transcription/data.py:140: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /Users/ning/anaconda3/envs/audio2midi/lib/python3.7/site-packages/magenta/models/onsets_frames_transcription/data.py:632: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CHECKPOINT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d25576379332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m estimator = train_util.create_estimator(\n\u001b[0;32m---> 17\u001b[0;31m     config.model_fn, CHECKPOINT_DIR, hparams)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHECKPOINT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "config = configs.CONFIG_MAP['onsets_frames']\n",
    "hparams = config.hparams\n",
    "hparams.use_cudnn = False\n",
    "hparams.batch_size = 1\n",
    "\n",
    "examples = tf.placeholder(tf.string, [None])\n",
    "\n",
    "dataset = data.provide_batch(\n",
    "    examples=examples,\n",
    "    preprocess_examples=True,\n",
    "    params=hparams,\n",
    "    is_training=False,\n",
    "    shuffle_examples=False,\n",
    "    skip_n_initial_records=0)\n",
    "\n",
    "estimator = train_util.create_estimator(\n",
    "    config.model_fn, CHECKPOINT_DIR, hparams)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_record = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cedfb236a522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m prediction_list = list(\n\u001b[0;32m----> 2\u001b[0;31m     estimator.predict(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         yield_single_examples=False))\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimator' is not defined"
     ]
    }
   ],
   "source": [
    "prediction_list = list(\n",
    "    estimator.predict(\n",
    "        input_fn,\n",
    "        yield_single_examples=False))\n",
    "assert len(prediction_list) == 1\n",
    "\n",
    "frame_predictions = prediction_list[0]['frame_predictions'][0]\n",
    "onset_predictions = prediction_list[0]['onset_predictions'][0]\n",
    "velocity_values = prediction_list[0]['velocity_values'][0]\n",
    "\n",
    "sequence_prediction = sequences_lib.pianoroll_to_note_sequence(\n",
    "    frame_predictions,\n",
    "    frames_per_second=data.hparams_frames_per_second(hparams),\n",
    "    min_duration_ms=0,\n",
    "    min_midi_pitch=constants.MIN_MIDI_PITCH,\n",
    "    onset_predictions=onset_predictions,\n",
    "    velocity_values=velocity_values)\n",
    "\n",
    "# Ignore warnings caused by pyfluidsynth\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "mm.plot_sequence(sequence_prediction)\n",
    "mm.play_sequence(sequence_prediction, mm.midi_synth.fluidsynth,\n",
    "                 colab_ephemeral=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:audio2midi]",
   "language": "python",
   "name": "conda-env-audio2midi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
